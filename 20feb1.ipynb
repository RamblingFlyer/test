{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "Error loading rainfall data: [Errno 2] No such file or directory: '/Users/aditya/Desktop/anoushka/Rainfall_Data_LL.csv'\n",
      "Error loading water level data: [Errno 2] No such file or directory: '/Users/aditya/Desktop/anoushka/Rajasthan_Water_Level.xlsx'\n",
      "Error in main execution: One or more datasets failed to load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/nt/q7hgyc8x3gz788mgyt5hmh2h0000gn/T/ipykernel_23153/4204895364.py\", line 454, in main\n",
      "    raise ValueError(\"One or more datasets failed to load\")\n",
      "ValueError: One or more datasets failed to load\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def load_rainfall_data(file_path):\n",
    "        \"\"\"Load and preprocess rainfall data\"\"\"\n",
    "        try:\n",
    "            rainfall_df = pd.read_csv(file_path)\n",
    "            rainfall_features = rainfall_df.groupby('YEAR').agg({\n",
    "                'ANNUAL': 'mean',\n",
    "                'June-September': 'mean',\n",
    "                'Mar-May': 'mean',\n",
    "                'Jan-Feb': 'mean',\n",
    "                'Oct-Dec': 'mean'\n",
    "            }).reset_index()\n",
    "            print(\"Rainfall data loaded successfully\")\n",
    "            return rainfall_features\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading rainfall data: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_canal_data(file_path):\n",
    "        \"\"\"Load and preprocess canal data\"\"\"\n",
    "        try:\n",
    "            canal_df = pd.read_csv(file_path)\n",
    "            canal_stats = pd.DataFrame({\n",
    "                'Canal_Flow_Mean': [canal_df['Canal Flow'].mean()],\n",
    "                'Canal_Flow_Std': [canal_df['Canal Flow'].std()],\n",
    "                'Soil_Moisture_Mean': [canal_df['Soil Moisture'].mean()],\n",
    "                'Aquifer_Thickness_Mean': [canal_df['Aquifer Thickness'].mean()],\n",
    "                'Hydraulic_Conductivity_Mean': [canal_df['Hydraulic Conductivity'].mean()]\n",
    "            })\n",
    "            print(\"Canal data loaded successfully\")\n",
    "            return canal_stats\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading canal data: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_water_level_data(file_path):\n",
    "        \"\"\"Load and preprocess water level data\"\"\"\n",
    "        try:\n",
    "            water_level_df = pd.read_excel(file_path)\n",
    "            processed_data = []\n",
    "\n",
    "            for year in range(2016, 2024):\n",
    "                if year == 2022:\n",
    "                    pre_col = f'Pre_{year}'\n",
    "                    post_col = 'Post_2022'\n",
    "                elif year == 2023:\n",
    "                    pre_col = f'Pre_{year}'\n",
    "                    post_col = 'Post_2023'\n",
    "                else:\n",
    "                    pre_col = f'Pre_{year}'\n",
    "                    post_col = f'Pst_{year}'\n",
    "\n",
    "                if pre_col not in water_level_df.columns or post_col not in water_level_df.columns:\n",
    "                    print(f\"Warning: Missing data for year {year}\")\n",
    "                    continue\n",
    "\n",
    "                year_data = water_level_df[[\n",
    "                    'Latitude', 'Longitude', 'Well_Depth',\n",
    "                    pre_col, post_col\n",
    "                ]].copy()\n",
    "\n",
    "                year_data[pre_col] = pd.to_numeric(year_data[pre_col].replace(' ', np.nan), errors='coerce')\n",
    "                year_data[post_col] = pd.to_numeric(year_data[post_col].replace(' ', np.nan), errors='coerce')\n",
    "\n",
    "                year_data['Year'] = year\n",
    "                year_data['Water_Level'] = (year_data[pre_col] + year_data[post_col]) / 2\n",
    "\n",
    "                year_data = year_data.dropna(subset=['Water_Level'])\n",
    "\n",
    "                if len(year_data) > 0:\n",
    "                    year_data = year_data.drop([pre_col, post_col], axis=1)\n",
    "                    processed_data.append(year_data)\n",
    "\n",
    "            if processed_data:\n",
    "                final_df = pd.concat(processed_data, ignore_index=True)\n",
    "                print(f\"Water level data loaded successfully. Total records: {len(final_df)}\")\n",
    "                return final_df\n",
    "            else:\n",
    "                raise ValueError(\"No valid data processed\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading water level data: {e}\")\n",
    "            return None\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def preprocess_data(self, rainfall_df, canal_df, water_level_df):\n",
    "        try:\n",
    "            if rainfall_df is None or canal_df is None or water_level_df is None:\n",
    "                raise ValueError(\"One or more input DataFrames are None\")\n",
    "\n",
    "            print(\"\\nInitial shapes:\")\n",
    "            print(f\"Rainfall data: {rainfall_df.shape}\")\n",
    "            print(f\"Canal data: {canal_df.shape}\")\n",
    "            print(f\"Water level data: {water_level_df.shape}\")\n",
    "\n",
    "            rainfall_df = rainfall_df.reset_index(drop=True)\n",
    "            water_level_df = water_level_df.reset_index(drop=True)\n",
    "            rainfall_df['YEAR'] = rainfall_df['YEAR'].astype(int)\n",
    "            water_level_df['Year'] = water_level_df['Year'].astype(int)\n",
    "\n",
    "            merged_data = pd.merge(\n",
    "                water_level_df,\n",
    "                rainfall_df[['YEAR', 'ANNUAL', 'June-September', 'Mar-May', 'Jan-Feb', 'Oct-Dec']],\n",
    "                left_on='Year',\n",
    "                right_on='YEAR',\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            for col in canal_df.columns:\n",
    "                merged_data[col] = canal_df[col].iloc[0]\n",
    "\n",
    "            merged_data = merged_data.fillna(method='ffill').fillna(method='bfill')\n",
    "            merged_data['MonthOfYear'] = 6\n",
    "            merged_data['DayOfYear'] = 182\n",
    "\n",
    "            feature_columns = [\n",
    "                'Year', 'MonthOfYear', 'DayOfYear',\n",
    "                'Latitude', 'Longitude', 'Well_Depth',\n",
    "                'ANNUAL', 'June-September', 'Mar-May',\n",
    "                'Jan-Feb', 'Oct-Dec'\n",
    "            ] + [col for col in merged_data.columns if 'Canal' in col or 'Soil' in col]\n",
    "\n",
    "            existing_columns = [col for col in feature_columns if col in merged_data.columns]\n",
    "\n",
    "            X = merged_data[existing_columns].copy()\n",
    "            y = merged_data['Water_Level'].copy()\n",
    "\n",
    "            print(\"\\nPreprocessing completed successfully\")\n",
    "            print(f\"Final shapes - X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data preprocessing: {e}\")\n",
    "            return None, None\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self):\n",
    "        self.rf_best_params = None\n",
    "        self.xgb_best_params = None\n",
    "        # self.lstm_best_params = None\n",
    "\n",
    "    def tune_random_forest(self, X_train, y_train):\n",
    "        \"\"\"Quick Random Forest tuning\"\"\"\n",
    "        print(\"\\nTuning Random Forest...\")\n",
    "\n",
    "        # Very focused parameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2],\n",
    "            'max_features': ['sqrt']\n",
    "        }\n",
    "\n",
    "        # Initialize RF model\n",
    "        rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "        # Quick grid search with minimal CV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=rf_model,\n",
    "            param_grid=param_grid,\n",
    "            cv=3,  # Reduced from 5 to 3\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        self.rf_best_params = grid_search.best_params_\n",
    "\n",
    "        print(\"Best RF Parameters:\", self.rf_best_params)\n",
    "        print(\"Best RF Score:\", np.sqrt(-grid_search.best_score_))\n",
    "\n",
    "        return self.rf_best_params\n",
    "\n",
    "    def tune_xgboost(self, X_train, y_train):\n",
    "        \"\"\"Quick XGBoost tuning using manual validation\"\"\"\n",
    "        print(\"\\nTuning XGBoost...\")\n",
    "\n",
    "        # Define parameter combinations\n",
    "        param_combinations = [\n",
    "            {\n",
    "                'n_estimators': 100,\n",
    "                'max_depth': 3,\n",
    "                'learning_rate': 0.01,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'min_child_weight': 1\n",
    "            },\n",
    "            {\n",
    "                'n_estimators': 200,\n",
    "                'max_depth': 5,\n",
    "                'learning_rate': 0.1,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'min_child_weight': 1\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        best_score = float('inf')\n",
    "        best_params = None\n",
    "\n",
    "        # Manual cross-validation\n",
    "        for params in param_combinations:\n",
    "            try:\n",
    "                # Create and train model\n",
    "                model = XGBRegressor(**params, random_state=42,n_estimators=500)\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_train, y_train)],\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Make predictions\n",
    "                predictions = model.predict(X_train)\n",
    "\n",
    "                # Calculate score\n",
    "                score = mean_squared_error(y_train, predictions)\n",
    "\n",
    "                # Update best parameters if better score found\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = params\n",
    "\n",
    "                print(f\"Parameters: {params}\")\n",
    "                print(f\"MSE Score: {score:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters {params}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if best_params is None:\n",
    "            print(\"Warning: Using default XGBoost parameters\")\n",
    "            best_params = {\n",
    "                'n_estimators': 100,\n",
    "                'max_depth': 3,\n",
    "                'learning_rate': 0.1,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'min_child_weight': 1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "\n",
    "        print(\"\\nBest XGBoost Parameters:\", best_params)\n",
    "        print(\"Best MSE Score:\", best_score)\n",
    "\n",
    "        self.xgb_best_params = best_params\n",
    "        return self.xgb_best_params\n",
    "\n",
    "    # def tune_lstm(self, X_train, y_train, X_val, y_val):\n",
    "    #     \"\"\"Quick LSTM tuning with fixed parameters\"\"\"\n",
    "    #     print(\"\\nConfiguring LSTM...\")\n",
    "\n",
    "    #     self.lstm_best_params = {\n",
    "    #         'lstm_units_1': 64,\n",
    "    #         'lstm_units_2': 32,\n",
    "    #         'dense_units': 16,\n",
    "    #         'dropout_1': 0.2,\n",
    "    #         'dropout_2': 0.2,\n",
    "    #         'learning_rate': 0.001,\n",
    "    #         'batch_size': 32\n",
    "    #     }\n",
    "\n",
    "    #     print(\"Using default LSTM parameters:\", self.lstm_best_params)\n",
    "    #     return self.lstm_best_params\n",
    "\n",
    "\n",
    "class ImprovedStackedEnsemble(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, rf_params=None, xgb_params=None):\n",
    "        self.rf_params = rf_params if rf_params else {'random_state': 42}\n",
    "        self.xgb_params = xgb_params if xgb_params else {'random_state': 42}\n",
    "        # self.lstm_params = lstm_params if lstm_params else {\n",
    "        #     'lstm_units_1': 64,\n",
    "        #     'lstm_units_2': 32,\n",
    "        #     'dense_units': 16,\n",
    "        #     'dropout_1': 0.2,\n",
    "        #     'dropout_2': 0.2,\n",
    "        #     'learning_rate': 0.001,\n",
    "        #     'batch_size': 32\n",
    "        # }\n",
    "\n",
    "        self.rf_model = RandomForestRegressor(**self.rf_params)\n",
    "        self.xgb_model = XGBRegressor(**self.xgb_params)\n",
    "        # self.lstm_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    # def prepare_lstm_data(self, X, rf_pred, xgb_pred, y=None, lookback=3):\n",
    "    #     \"\"\"Prepare sequences for LSTM with proper pandas handling\"\"\"\n",
    "    #     X_seq, y_seq = [], []\n",
    "\n",
    "    #     # Convert inputs to numpy arrays\n",
    "    #     X_array = X.values if isinstance(X, pd.DataFrame) else np.array(X)\n",
    "    #     rf_pred_array = rf_pred.reshape(-1, 1)\n",
    "    #     xgb_pred_array = xgb_pred.reshape(-1, 1)\n",
    "    #     combined_features = np.column_stack([X_array, rf_pred_array, xgb_pred_array])\n",
    "\n",
    "    #     if y is not None:\n",
    "    #         y_array = y.values if isinstance(y, pd.Series) else np.array(y)\n",
    "\n",
    "    #     for i in range(len(combined_features) - lookback):\n",
    "    #         X_seq.append(combined_features[i:(i + lookback)])\n",
    "    #         if y is not None:\n",
    "    #             y_seq.append(y_array[i + lookback])\n",
    "\n",
    "    #     X_seq = np.array(X_seq)\n",
    "    #     if y is not None:\n",
    "    #         y_seq = np.array(y_seq)\n",
    "    #         return X_seq, y_seq\n",
    "    #     return X_seq\n",
    "\n",
    "    # def build_lstm(self, input_shape):\n",
    "    #     \"\"\"Build LSTM model\"\"\"\n",
    "    #     model = Sequential([\n",
    "    #         LSTM(self.lstm_params['lstm_units_1'],\n",
    "    #              activation='relu',\n",
    "    #              input_shape=input_shape,\n",
    "    #              return_sequences=True),\n",
    "    #         Dropout(self.lstm_params['dropout_1']),\n",
    "    #         LSTM(self.lstm_params['lstm_units_2'],\n",
    "    #              activation='relu'),\n",
    "    #         Dropout(self.lstm_params['dropout_2']),\n",
    "    #         Dense(self.lstm_params['dense_units'],\n",
    "    #              activation='relu'),\n",
    "    #         Dense(1)\n",
    "    #     ])\n",
    "\n",
    "    #     model.compile(\n",
    "    #         optimizer=tf.keras.optimizers.Adam(\n",
    "    #             learning_rate=self.lstm_params['learning_rate']\n",
    "    #         ),\n",
    "    #         loss='mse'\n",
    "    #     )\n",
    "    #     return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the stacked ensemble\"\"\"\n",
    "        print(\"\\nTraining Level 1 - Random Forest...\")\n",
    "        self.rf_model.fit(X, y)\n",
    "        rf_predictions = self.rf_model.predict(X)\n",
    "        print(rf_predictions)\n",
    "        print(\"Training Level 2 - XGBoost...\")\n",
    "        xgb_features = np.column_stack([X, rf_predictions])\n",
    "        print(xgb_features)\n",
    "        self.xgb_model.fit(xgb_features, y)\n",
    "        xgb_predictions = self.xgb_model.predict(xgb_features)\n",
    "\n",
    "        # print(\"Training Level 3 - LSTM...\")\n",
    "        # X_lstm, y_lstm = self.prepare_lstm_data(X, rf_predictions, xgb_predictions, y)\n",
    "        # self.lstm_model = self.build_lstm((X_lstm.shape[1], X_lstm.shape[2]))\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # self.lstm_model.fit(\n",
    "        #     X_lstm, y_lstm,\n",
    "        #     epochs=20,\n",
    "        #     batch_size=self.lstm_params['batch_size'],\n",
    "        #     validation_split=0.2,\n",
    "        #     callbacks=[early_stopping],\n",
    "        #     verbose=1\n",
    "        # )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate predictions\"\"\"\n",
    "        rf_predictions = self.rf_model.predict(X)\n",
    "        xgb_features = np.column_stack([X, rf_predictions])\n",
    "        xgb_predictions = self.xgb_model.predict(xgb_features)\n",
    "        # X_lstm = self.prepare_lstm_data(X, rf_predictions, xgb_predictions)\n",
    "\n",
    "        # if len(X_lstm) > 0:\n",
    "        #     return self.lstm_model.predict(X_lstm).flatten()\n",
    "        return rf_predictions\n",
    "\n",
    "    def predict_future(self, X_last, years=5):\n",
    "        \"\"\"Predict future values\"\"\"\n",
    "        future_predictions = []\n",
    "        current_data = X_last.copy()\n",
    "\n",
    "        for year in range(years):\n",
    "          # 1. Make prediction for current year\n",
    "          pred = self.predict(current_data)\n",
    "          future_predictions.extend(pred)  # Store prediction\n",
    "\n",
    "          # 2. Update data for next year's prediction:\n",
    "          current_data = np.roll(current_data, -1, axis=0)  # Shift data one step\n",
    "          current_data[-1, 0] = 2024 + year + 1  # Update Year\n",
    "          current_data[-1, 3:] = current_data[-2, 3:]  # Hold Latitude, Longitude, Well_Depth constant\n",
    "          current_data[-1, 6:11] = current_data[-2, 6:11]  # Hold rainfall features constant\n",
    "          current_data[-1, 11:] = current_data[-2, 11:]  # Hold canal features constant\n",
    "          current_data[-1, -2] = pred[-1]  # Update with the predicted water level\n",
    "        return future_predictions\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error in prediction pipeline: {e}\")\n",
    "        #     return np.full(years, last_prediction)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate R² score\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return r2_score(y[-len(predictions):], predictions)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        rf_predictions = self.rf_model.predict(X_test)\n",
    "        xgb_features = np.column_stack([X_test, rf_predictions])\n",
    "        xgb_predictions = self.xgb_model.predict(xgb_features)\n",
    "        final_predictions = self.predict(X_test)\n",
    "\n",
    "        return {\n",
    "            'RF_MSE': mean_squared_error(y_test, rf_predictions),\n",
    "            'RF_R2': r2_score(y_test, rf_predictions),\n",
    "            'RF_MAE': mean_absolute_error(y_test, rf_predictions),\n",
    "            'XGB_MSE': mean_squared_error(y_test, xgb_predictions),\n",
    "            'XGB_R2': r2_score(y_test, xgb_predictions),\n",
    "            'XGB_MAE': mean_absolute_error(y_test, xgb_predictions)\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load data\n",
    "        data_loader = DataLoader()\n",
    "        print(\"\\nLoading data...\")\n",
    "        rainfall_data = data_loader.load_rainfall_data('/Users/aditya/Desktop/anoushka/test/Rainfall_Data_LL.csv')\n",
    "        canal_data = data_loader.load_canal_data('/Users/aditya/Desktop/anoushka/test/CleanedCanalData.csv')\n",
    "        water_level_data = data_loader.load_water_level_data('/Users/aditya/Desktop/anoushka/test/Rajasthan_Water_Level.xlsx')\n",
    "\n",
    "        if rainfall_data is None or canal_data is None or water_level_data is None:\n",
    "            raise ValueError(\"One or more datasets failed to load\")\n",
    "\n",
    "        # Preprocess\n",
    "        preprocessor = DataPreprocessor()\n",
    "        print(\"\\nPreprocessing data...\")\n",
    "        X, y = preprocessor.preprocess_data(rainfall_data, canal_data, water_level_data)\n",
    "        print(\"\\nPreprocessed Data (X):\")\n",
    "        print(X.head())  # Display the first few rows of X\n",
    "        print(\"\\nPreprocessed Target (y):\")\n",
    "        print(y.head())\n",
    "        if X is None or y is None:\n",
    "            raise ValueError(\"Preprocessing failed to produce valid output\")\n",
    "\n",
    "        # Take a smaller sample for tuning\n",
    "        sample_size = min(3000, len(X))\n",
    "        indices = np.random.choice(len(X), sample_size, replace=False)\n",
    "        X_sample = X.iloc[indices]\n",
    "        y_sample = y.iloc[indices]\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_sample, y_sample, test_size=0.2, random_state=42\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Quick tuning\n",
    "        print(\"\\nStarting quick hyperparameter tuning...\")\n",
    "        tuner = HyperparameterTuner()\n",
    "        rf_params = tuner.tune_random_forest(X_train_scaled, y_train)\n",
    "        xgb_params = tuner.tune_xgboost(X_train_scaled, y_train)\n",
    "        # lstm_params = tuner.tune_lstm(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "\n",
    "        # Train final model\n",
    "        print(\"\\nTraining final model with tuned parameters...\")\n",
    "        model = ImprovedStackedEnsemble(rf_params, xgb_params)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        metrics = model.evaluate(X_test_scaled, y_test)\n",
    "        print(\"\\nModel Performance Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Generate predictions\n",
    "        print(\"\\nGenerating future predictions...\")\n",
    "        future_preds =enumerate(model.predict_future(X_test_scaled[-3:],years=5),1)\n",
    "        current_year=2024\n",
    "        print(\"\\nPredictions for next 5 years:\")\n",
    "        for i, pred in future_preds:\n",
    "          if current_year + i <= 2029:  # Print only up to 2029\n",
    "            # print(f\"Year {current_year + i}: {pred:.2f}m\")\n",
    "            print(f\"Year {2024 + i}: {pred:.2f}m\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
